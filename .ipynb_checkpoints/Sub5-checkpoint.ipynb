{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Multi-class and Multi-Label Classification Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Download the Anuran Calls (MFCCs) Data Set from: https://archive.ics.uci.edu/ml/datasets/Anuran+Calls+%28MFCCs%29. Choose 70% of the data randomly as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Frogs_MFCCs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, train_size=0.7, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Each instance has three labels: Families, Genus, and Species. Each of the labels has multiple classes. We wish to solve a multi-class and multi-label problem. One of the most important approaches to multi-class classification is to train a classifier for each label. We first try this approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Research exact match and hamming score/ loss methods for evaluating multi-label classification and use them in evaluating the classifiers in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MFCCs_ 1', 'MFCCs_ 2', 'MFCCs_ 3', 'MFCCs_ 4', 'MFCCs_ 5',\n",
       "       'MFCCs_ 6', 'MFCCs_ 7', 'MFCCs_ 8', 'MFCCs_ 9', 'MFCCs_10',\n",
       "       'MFCCs_11', 'MFCCs_12', 'MFCCs_13', 'MFCCs_14', 'MFCCs_15',\n",
       "       'MFCCs_16', 'MFCCs_17', 'MFCCs_18', 'MFCCs_19', 'MFCCs_20',\n",
       "       'MFCCs_21', 'MFCCs_22', 'Family', 'Genus', 'Species', 'RecordID'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['Family', 'Genus', 'Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.iloc[:, :-4]\n",
    "x_test = test_data.iloc[:, :-4]\n",
    "y_train = train_data.iloc[:, -4:-1]\n",
    "y_test = test_data.iloc[:, -4:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing Family**\n",
      "Hamming Loss is 0.07920333487725799\n",
      "Exact Match score is 0.920796665122742\n",
      "\n",
      "\n",
      "**Processing Genus**\n",
      "Hamming Loss is 0.08337193144974525\n",
      "Exact Match score is 0.9166280685502547\n",
      "\n",
      "\n",
      "**Processing Species**\n",
      "Hamming Loss is 0.07086614173228346\n",
      "Exact Match score is 0.9291338582677166\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "for category in categories:\n",
    "    print('**Processing {}**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, train_data[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Hamming Loss is {}'.format(hamming_loss(test_data[category], prediction)))\n",
    "    print('Exact Match score is {}'.format(accuracy_score(test_data[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Train a SVM for each of the labels, using Gaussian kernels and one versus all classifiers. Determine the weight of the SVM penalty and the width of the Gaussian Kernel using 10 fold cross validation. You are welcome to try to solve the problem with both standardized and raw attributes and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'C':np.logspace(-5, 8, 10), \n",
    "              'gamma': np.append(np.logspace(-4, -1, 10), np.logspace(0, 2, 10))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing Family**\n",
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "{'C': 10}\n",
      "0.9529388403494837\n",
      "Hamming Loss is 0.05094951366373321\n",
      "Exact Match score is 0.9490504863362668\n",
      "\n",
      "\n",
      "**Processing Genus**\n",
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "{'C': 10}\n",
      "0.9521445591739476\n",
      "Hamming Loss is 0.04029643353404354\n",
      "Exact Match score is 0.9597035664659564\n",
      "\n",
      "\n",
      "**Processing Species**\n",
      "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "{'C': 10}\n",
      "0.9666401906274821\n",
      "Hamming Loss is 0.031496062992125984\n",
      "Exact Match score is 0.968503937007874\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame()\n",
    "for category in categories:\n",
    "    print('**Processing {}**'.format(category))\n",
    "    svc = SVC(kernel='rbf', decision_function_shape='ovr')\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    clf = GridSearchCV(svc, parameters, cv=kf, scoring='accuracy')\n",
    "    clf.fit(x_train, train_data[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_true = test_data[category]\n",
    "    pred[category] = y_pred\n",
    "    print('Best Parameter {}'.format(clf.best_params_))\n",
    "    print('Precision {}'.format(metrics.precision_score(y_true, y_pred, average='micro')))\n",
    "    print('Recall {}'.format(metrics.recall_score(y_true, y_pred, average='micro')))\n",
    "    print('F1 Score {}'.format(metrics.f1_score(y_true, y_pred, average='micro')))\n",
    "    print('Hamming Loss is {}'.format(hamming_loss(y_true, y_pred)))\n",
    "    print('Exact Match score is {}'.format(accuracy_score(y_true, y_pred)))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction for Test data:\\n', pred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(pred)\n",
    "def compute_hamming_loss(pred):\n",
    "    hamming_loss = []\n",
    "    for l in categories:\n",
    "        hamming_loss.append(metrics.hamming_loss(test_df[l], pred[l]))\n",
    "    return np.mean(hamming_loss)\n",
    "print(\"The hamming loss for Gaussian kernel SVM is\", compute_hamming_loss(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Repeat 1(b)ii and L1-penalized SVMs. Remember to standardize the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Processing Family**\n",
      "Best Parameter {'C': 10}\n",
      "Best score 0.9392374900714853\n",
      "Confusion matrix\n",
      "[[   0    0   15    2]\n",
      " [   0  132    8    7]\n",
      " [   1    5  576   62]\n",
      " [   0   18   32 1301]]\n",
      "Precision 0.9305233904585456\n",
      "Recall 0.9305233904585456\n",
      "F1 Score 0.9305233904585456\n",
      "Hamming Loss is 0.06947660954145438\n",
      "Exact Match score is 0.9305233904585456\n",
      "\n",
      "\n",
      "**Processing Genus**\n",
      "Best Parameter {'C': 10}\n",
      "Best score 0.9529388403494837\n",
      "Confusion matrix\n",
      "[[1242   15    2    4    0    0    0    2]\n",
      " [   5  138    2    2    0    0    0    0]\n",
      " [  28    5   53    6    0    0    0    1]\n",
      " [  13    0    0  463    0    0    0    0]\n",
      " [   0    0    3    7   76    0    0    0]\n",
      " [   1    0    0   12    0   15    0    0]\n",
      " [   2    0    0    3    1    0   11    0]\n",
      " [   0    0    0    1    0    0    0   46]]\n",
      "Precision 0.9467345993515517\n",
      "Recall 0.9467345993515517\n",
      "F1 Score 0.9467345993515517\n",
      "Hamming Loss is 1.0\n",
      "Exact Match score is 0.0\n",
      "\n",
      "\n",
      "**Processing Species**\n",
      "Best Parameter {'C': 10}\n",
      "Best score 0.9594916600476568\n",
      "Confusion matrix\n",
      "[[ 188    0   12    0    0    3    0    0    0    1]\n",
      " [   0 1060    0    0    1    0    0    0    0    0]\n",
      " [   1    0  140    1    1    3    0    0    1    0]\n",
      " [  12    8    7   61    0    4    0    0    0    1]\n",
      " [   4    0    0    0  133    3    0    0    0    0]\n",
      " [   0    2    0    0    6  324    2    0    2    0]\n",
      " [   1    0    0    3    0    1   81    0    0    0]\n",
      " [   0    0    0    0    2    6    0   17    3    0]\n",
      " [   1    0    1    0    0    2    1    0   12    0]\n",
      " [   0    0    0    0    0    0    0    0    0   47]]\n",
      "Precision 0.9555349698934692\n",
      "Recall 0.9555349698934692\n",
      "F1 Score 0.9555349698934692\n",
      "Hamming Loss is 1.0\n",
      "Exact Match score is 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred2 = pd.DataFrame()\n",
    "for category in categories:\n",
    "    print('**Processing {}**'.format(category))\n",
    "    svc = LinearSVC(penalty='l1', multi_class='ovr', dual=False)\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    clf = GridSearchCV(svc, parameters, cv=kf, scoring='accuracy')\n",
    "    clf.fit(x_train, train_data[category])\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_true = test_data[category]\n",
    "    pred2[category] = y_pred \n",
    "    print('Best Parameter {}'.format(clf.best_params_))\n",
    "    print('Precision {}'.format(metrics.precision_score(y_true, y_pred, average='micro')))\n",
    "    print('Recall {}'.format(metrics.recall_score(y_true, y_pred, average='micro')))\n",
    "    print('F1 Score {}'.format(metrics.f1_score(y_true, y_pred, average='micro')))\n",
    "    print('Hamming Loss is {}'.format(hamming_loss(test_data[category], prediction)))\n",
    "    print('Exact Match score is {}'.format(accuracy_score(test_data[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-e2904f638818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Prediction for Test data:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pred2' is not defined"
     ]
    }
   ],
   "source": [
    "print('Prediction for Test data:\\n', pred2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = pd.DataFrame(pred2)\n",
    "print(\"The hamming loss for linear SVM is\", compute_hamming_loss(pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Repeat 1(b)iii by using SMOTE or any other method you know to remedy class imbalance. Report your conclusions about the classifiers you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=2)\n",
    "\n",
    "pred2 = pd.DataFrame()\n",
    "for category in categories:\n",
    "    print('**Processing {}**'.format(category))\n",
    "    svc = LinearSVC(penalty='l1', multi_class='ovr', dual=False)\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "    clf = GridSearchCV(svc, parameters, cv=kf, scoring='accuracy')\n",
    "    x_train_res, y_train_res = sm.fit_sample(x_train, train_data[category])\n",
    "    clf.fit(x_train_res, y_train_res)\n",
    "    \n",
    "    # calculating test accuracy\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_true = test_data[category]\n",
    "    pred2[category] = y_pred \n",
    "    print('Best Parameter {}'.format(clf.best_params_))\n",
    "    print('Precision {}'.format(metrics.precision_score(y_true, y_pred, average='micro')))\n",
    "    print('Recall {}'.format(metrics.recall_score(y_true, y_pred, average='micro')))\n",
    "    print('F1 Score {}'.format(metrics.f1_score(y_true, y_pred, average='micro')))\n",
    "    print('Hamming Loss is {}'.format(hamming_loss(test_data[category], prediction)))\n",
    "    print('Exact Match score is {}'.format(accuracy_score(test_data[category], prediction)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v. Extra Practice: Study the Classifier Chain method and apply it to the above problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "svc = LinearSVC(penalty='l1', multi_class='ovr', dual=False)\n",
    "\n",
    "# Fit an ensemble of logistic regression classifier chains and take the\n",
    "# take the average prediction of all the chains.\n",
    "chains = [ClassifierChain(svc, order='random', random_state=5)\n",
    "          for i in range(10)]\n",
    "for chain in chains:\n",
    "    chain.fit(x_train, y_train)\n",
    "\n",
    "Y_pred_chains = np.array([chain.predict(x_test) for chain in\n",
    "                          chains])\n",
    "chain_jaccard_scores = [jaccard_similarity_score(y_test, Y_pred_chain >= .5)\n",
    "                        for Y_pred_chain in Y_pred_chains]\n",
    "\n",
    "print(Y_pred_chains.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vi. Extra Practice: Research how confusion matrices, precision, recall, ROC, and AUC are defined for multi-label classification and compute them for the classifiers you trained in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. K-Means Clustering on a Multi-Class and Multi-Label Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Simulation: Perform the following procedures 50 times, and report the average and standard deviation of the 50 Hamming Distances that you calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Use k-means clustering on the whole Anuran Calls (MFCCs) Data Set (do not split the data into train and test, as we are not performing supervised learning in this exercise). Choose k automatically based on one of the methods provided in the slides (CH or Gap Statistics or scree plots or Silhouettes) or any other method you know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-6a7ed0ee36cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mkmeanModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeanModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0msilh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msilhouette_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mdistortions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkmeanModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shraddha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msilhouette_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\shraddha\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\metrics\\cluster\\unsupervised.py\u001b[0m in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_samples_curr_lab\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             intra_clust_dists[mask] = np.sum(\n\u001b[1;32m--> 192\u001b[1;33m                 current_distances[:, mask], axis=1) / n_samples_curr_lab\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;31m# Now iterate over all other labels, finding the mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# clustering dataset\n",
    "# determine k using elbow method\n",
    "# k means determine k\n",
    "ks = []\n",
    "dist = []\n",
    "for i in tqdm(range(50)):\n",
    "    diss = []\n",
    "    silh = []\n",
    "    results = []\n",
    "    X = df.iloc[:, :-4]\n",
    "    K = range(2,15)\n",
    "    for k in range(2, 15):\n",
    "        kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "        label = kmeanModel.labels_\n",
    "        results.append(label)\n",
    "        silh.append(silhouette_score(X, label))\n",
    "        diss.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "    index = np.argmax(silh)\n",
    "    best_k = index + 2\n",
    "    ks.append(best_k)\n",
    "    dist.append(diss)\n",
    "    best_labels = results[index]\n",
    "\n",
    "    print('The values of k is', mode(ks))\n",
    "\n",
    "\"\"\"distortions = []\n",
    "for i in range(len(dist)):\n",
    "    av = 0\n",
    "    for j in range(len(dist)):\n",
    "         av += dist[j][i]\n",
    "    distortions.append(av/len(dist))\"\"\"\n",
    "    \n",
    "    \n",
    "    label_df = df[['Family', 'Genus', 'Species']].copy()\n",
    "    labels_df['kmeans_label'] = best_labels\n",
    "    majority_label = {}\n",
    "    for l in range(best_k):\n",
    "        cluster = labels_df[labels_df['kmeans_label'] == l]\n",
    "        majority_triplet = {}\n",
    "        for tl in ['Family', 'Genus', 'Species']:\n",
    "            majority_triplet[tl] = cluster[tl].value_counts().idxmax()\n",
    "        majority_label[l] = majority_triplet\n",
    "    print(\"Majority labels:\\n\", majority_label)\n",
    "\n",
    "    #compute hamming loss\n",
    "    misclassifed = 0\n",
    "    for l in range(best_k):\n",
    "        cluster = labels_df[labels_df['kmeans_label'] == l]\n",
    "        for tl in ['Family', 'Genus', 'Species']:\n",
    "            misclassifed += sum(cluster[tl] != majority_label[l][tl])\n",
    "    hamming_loss = misclassifed / (len(anuran_df) * 3)\n",
    "    print('Hamming Loss:',hamming_loss)\n",
    "\n",
    "# Plot the elbow\n",
    "\"\"\"plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) In each cluster, determine which family is the majority by reading the true labels. Repeat for genus and species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Now for each cluster you have a majority label triplet (family, genus, species). Calculate the average Hamming distance (score) between the true labels and the labels assigned by clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
